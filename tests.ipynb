{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d15cf37",
   "metadata": {},
   "source": [
    "# Kite Connect Testing Notebook\n",
    "\n",
    "Test suite for Kite API integration, position analysis, and trading operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19f03d",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies and Initialize Kite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba92136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your main_strategy.py\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "969542de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kite object: <kiteconnect.connect.KiteConnect object at 0x00000204ABC30EC0>\n",
      "Type: <class 'kiteconnect.connect.KiteConnect'>\n",
      "\n",
      "‚úÖ UserID: XJY521\n",
      "‚úÖ User Name: Indhuja .\n",
      "‚úÖ Email: sathyakumarnandakumar@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# Reload the module to get fresh kite object with new token\n",
    "import importlib\n",
    "import load_kite_from_access\n",
    "#importlib.reload(load_kite_from_access)\n",
    "kite = load_kite_from_access.kite\n",
    "\n",
    "# Check kite status and get UserID\n",
    "print(f\"Kite object: {kite}\")\n",
    "print(f\"Type: {type(kite)}\")\n",
    "if kite:\n",
    "    try:\n",
    "        profile = kite.profile()\n",
    "        print(f\"\\n‚úÖ UserID: {profile['user_id']}\")\n",
    "        print(f\"‚úÖ User Name: {profile['user_name']}\")\n",
    "        print(f\"‚úÖ Email: {profile['email']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting profile: {e}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Kite object is None - session not established\")\n",
    "    print(\"Please ensure you have run config.py to generate access token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e5b4c",
   "metadata": {},
   "source": [
    "## 3. Fetch and Group Instruments (NSE + NFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f344a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching instruments from NSE...\n",
      "Fetching instruments from NFO...\n",
      "Fetching instruments from MCX...\n",
      "\n",
      "Total Instruments Fetched: 121,771\n",
      "  - NSE: 9,202\n",
      "  - NFO: 38,795\n",
      "\n",
      "Instrument Types: ['CE', 'EQ', 'FUT', 'PE']\n",
      "Segments: ['INDICES', 'MCX-FUT', 'MCX-OPT', 'NFO-FUT', 'NFO-OPT', 'NSE']\n",
      "\n",
      "============================================================\n",
      "INSTRUMENT GROUPING SUMMARY\n",
      "============================================================\n",
      "Equity (EQ):          9,068 instruments\n",
      "Indices:              145 instruments\n",
      "============================================================\n",
      "\n",
      " EQUITY SAMPLE (Top 5):\n",
      "    tradingsymbol                       name exchange instrument_type\n",
      "134   GOLDSTAR-SM             GOLDSTAR POWER      NSE              EQ\n",
      "135    21STCENMGM  21ST CENTURY MGMT SERVICE      NSE              EQ\n",
      "136      AARTIIND           AARTI INDUSTRIES      NSE              EQ\n",
      "137           ABB                  ABB INDIA      NSE              EQ\n",
      "138    656KA30-SG          SDL KA 6.56% 2030      NSE              EQ\n",
      "\n",
      " INDEX SAMPLE (Top 5):\n",
      "       tradingsymbol               name exchange\n",
      "0           NIFTY 50           NIFTY 50      NSE\n",
      "1   NIFTY MIDCAP 100   NIFTY MIDCAP 100      NSE\n",
      "2         NIFTY BANK         NIFTY BANK      NSE\n",
      "3          NIFTY 100          NIFTY 100      NSE\n",
      "4  NIFTY DIV OPPS 50  NIFTY DIV OPPS 50      NSE\n",
      "\n",
      "‚úÖ All instruments grouped and stored in pandas DataFrames:\n",
      "   - df_equity, df_index, df_futures, df_options_ce, df_options_pe\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 1. Fetch instruments from NSE (Equities & Indices) and NFO (Futures & Options)\n",
    "    print(\"Fetching instruments from NSE...\")\n",
    "    instruments_nse = kite.instruments(\"NSE\")\n",
    "    \n",
    "    print(\"Fetching instruments from NFO...\")\n",
    "    instruments_nfo = kite.instruments(\"NFO\")\n",
    "    \n",
    "    print(\"Fetching instruments from MCX...\")\n",
    "    instruments_mcx = kite.instruments(\"MCX\")\n",
    "\n",
    "    # 2. Combine and convert to DataFrame\n",
    "    instruments_all = instruments_nse + instruments_nfo + instruments_mcx\n",
    "    \n",
    "    df_all = pd.DataFrame(instruments_all)\n",
    "    \n",
    "    \n",
    "    print(f\"\\nTotal Instruments Fetched: {len(df_all):,}\")\n",
    "    print(f\"  - NSE: {len(instruments_nse):,}\")\n",
    "    print(f\"  - NFO: {len(instruments_nfo):,}\")\n",
    "    print(f\"\\nInstrument Types: {sorted(df_all['instrument_type'].unique())}\")\n",
    "    print(f\"Segments: {sorted(df_all['segment'].unique())}\")\n",
    "    \n",
    "    # 3. Group instruments by type\n",
    "    # Equity (EQ) - from NSE\n",
    "    df_equity = df_all[(df_all['instrument_type'] == 'EQ') & (df_all['segment'] == 'NSE')].copy()\n",
    "    \n",
    "    # Index - INDICES segment\n",
    "    df_index = df_all[df_all['segment'] == 'INDICES'].copy()\n",
    "    \n",
    "    # # Futures (FUT) - from NFO\n",
    "    # df_futures = df_all[df_all['instrument_type'] == 'FUT'].copy()\n",
    "    \n",
    "    # # Options - Call (CE)\n",
    "    # df_options_ce = df_all[df_all['instrument_type'] == 'CE'].copy()\n",
    "    \n",
    "    # # Options - Put (PE)\n",
    "    # df_options_pe = df_all[df_all['instrument_type'] == 'PE'].copy()\n",
    "    \n",
    "    # 4. Display summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INSTRUMENT GROUPING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Equity (EQ):          {len(df_equity):,} instruments\")\n",
    "    print(f\"Indices:              {len(df_index):,} instruments\")\n",
    "    # print(f\"Futures (FUT):        {len(df_futures):,} instruments\")\n",
    "    # print(f\"Options - Call (CE):  {len(df_options_ce):,} instruments\")\n",
    "    # print(f\"Options - Put (PE):   {len(df_options_pe):,} instruments\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 5. Show samples of each type\n",
    "    print(\"\\n EQUITY SAMPLE (Top 5):\")\n",
    "    if len(df_equity) > 0:\n",
    "        print(df_equity[['tradingsymbol', 'name', 'exchange', 'instrument_type']].head())\n",
    "    \n",
    "    print(\"\\n INDEX SAMPLE (Top 5):\")\n",
    "    if len(df_index) > 0:\n",
    "        print(df_index[['tradingsymbol', 'name', 'exchange']].head())\n",
    "    \n",
    "    # print(\"\\n FUTURES SAMPLE (Top 5):\")\n",
    "    # if len(df_futures) > 0:\n",
    "    #     print(df_futures[['tradingsymbol', 'name', 'expiry', 'lot_size', 'instrument_type']].head())\n",
    "    \n",
    "    # print(\"\\n OPTIONS CE SAMPLE (Top 5):\")\n",
    "    # if len(df_options_ce) > 0:\n",
    "    #     print(df_options_ce[['tradingsymbol', 'name', 'strike', 'expiry', 'lot_size']].head())\n",
    "    \n",
    "    # print(\"\\n OPTIONS PE SAMPLE (Top 5):\")\n",
    "    # if len(df_options_pe) > 0:\n",
    "    #     print(df_options_pe[['tradingsymbol', 'name', 'strike', 'expiry', 'lot_size']].head())\n",
    "    \n",
    "    print(\"\\n‚úÖ All instruments grouped and stored in pandas DataFrames:\")\n",
    "    print(\"   - df_equity, df_index, df_futures, df_options_ce, df_options_pe\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945fc3f",
   "metadata": {},
   "source": [
    "## Standardise Date and Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31e08509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STANDARDIZING DATA FORMATS\n",
      "================================================================================\n",
      "\n",
      "üìä NSE/NFO DataFrames:\n",
      "‚úÖ df_all: 121,771 rows standardized\n",
      "‚úÖ df_equity: 9,068 rows standardized\n",
      "‚úÖ df_index: 145 rows standardized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Standardize and format all dataframes for consistency\n",
    "print(\"=\"*80)\n",
    "print(\"STANDARDIZING DATA FORMATS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Helper function to standardize datetime columns\n",
    "def standardize_dataframe(df, name=\"DataFrame\"):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert date columns to datetime64\n",
    "    date_columns = ['expiry', 'last_date']\n",
    "    for col in date_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "    \n",
    "    # Ensure numeric columns are proper types\n",
    "    numeric_columns = ['strike', 'tick_size', 'lot_size']\n",
    "    for col in numeric_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    \n",
    "    # String columns should be proper strings (including tokens)\n",
    "    string_columns = ['tradingsymbol', 'name', 'exchange', 'segment', 'instrument_type', \n",
    "                      'instrument_token', 'exchange_token']\n",
    "    for col in string_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].astype(str)\n",
    "    \n",
    "    print(f\"‚úÖ {name}: {len(df_clean):,} rows standardized\")\n",
    "    return df_clean\n",
    "\n",
    "# Standardize all NSE/NFO dataframes\n",
    "print(\"\\nüìä NSE/NFO DataFrames:\")\n",
    "df_all = standardize_dataframe(df_all, \"df_all\")\n",
    "df_equity = standardize_dataframe(df_equity, \"df_equity\")\n",
    "df_index = standardize_dataframe(df_index, \"df_index\")\n",
    "# df_futures = standardize_dataframe(df_futures, \"df_futures\")\n",
    "# df_options_ce = standardize_dataframe(df_options_ce, \"df_options_ce\")\n",
    "# df_options_pe = standardize_dataframe(df_options_pe, \"df_options_pe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78632062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä MCX DataFrames:\n",
      "‚úÖ df_mcx: 73,774 rows standardized\n",
      "\n",
      "================================================================================\n",
      "DATA STANDARDIZATION COMPLETE\n",
      "================================================================================\n",
      "All dataframes now have:\n",
      "  - DateTime columns in datetime64 format\n",
      "  - Numeric columns (strike, tick_size, lot_size) in proper numeric types\n",
      "  - String columns (including instrument_token, exchange_token) as strings\n",
      "  - Ready for Parquet export!\n"
     ]
    }
   ],
   "source": [
    "# Standardize MCX dataframe\n",
    "print(\"\\nüìä MCX DataFrames:\")\n",
    "df_mcx = pd.DataFrame(instruments_mcx)\n",
    "df_mcx = standardize_dataframe(df_mcx, \"df_mcx\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA STANDARDIZATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"All dataframes now have:\")\n",
    "print(\"  - DateTime columns in datetime64 format\")\n",
    "print(\"  - Numeric columns (strike, tick_size, lot_size) in proper numeric types\")\n",
    "print(\"  - String columns (including instrument_token, exchange_token) as strings\")\n",
    "print(\"  - Ready for Parquet export!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846cda5a",
   "metadata": {},
   "source": [
    "## 4. Export Instruments to CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all DataFrames to Parquet files for efficient storage\n",
    "import os\n",
    "\n",
    "# Create a folder for the data\n",
    "output_folder = \"instruments_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save each DataFrame to Parquet (data already standardized in previous cell)\n",
    "files_created = []\n",
    "\n",
    "# NSE + NFO instruments\n",
    "df_all.to_parquet(f\"{output_folder}/df_all.parquet\", engine='pyarrow')\n",
    "files_created.append(f\"{output_folder}/df_all.parquet\")\n",
    "\n",
    "df_equity.to_parquet(f\"{output_folder}/equity.parquet\", engine='pyarrow')\n",
    "files_created.append(f\"{output_folder}/equity.parquet\")\n",
    "\n",
    "df_index.to_parquet(f\"{output_folder}/indices.parquet\", engine='pyarrow')\n",
    "files_created.append(f\"{output_folder}/indices.parquet\")\n",
    "\n",
    "df_futures.to_parquet(f\"{output_folder}/futures.parquet\", engine='pyarrow')\n",
    "files_created.append(f\"{output_folder}/futures.parquet\")\n",
    "\n",
    "df_options_ce.to_parquet(f\"{output_folder}/options_ce.parquet\", engine='pyarrow')\n",
    "files_created.append(f\"{output_folder}/options_ce.parquet\")\n",
    "\n",
    "df_options_pe.to_parquet(f\"{output_folder}/options_pe.parquet\", engine='pyarrow')\n",
    "files_created.append(f\"{output_folder}/options_pe.parquet\")\n",
    "\n",
    "# MCX dataframes (already created and standardized in previous cell)\n",
    "df_mcx.to_parquet(f\"{output_folder}/mcx_all.parquet\", engine='pyarrow')\n",
    "files_created.append(f\"{output_folder}/mcx_all.parquet\")\n",
    "\n",
    "df_mcx_futures.to_parquet(f\"{output_folder}/mcx_futures.parquet\", engine='pyarrow')\n",
    "files_created.append(f\"{output_folder}/mcx_futures.parquet\")\n",
    "\n",
    "df_mcx_ce.to_parquet(f\"{output_folder}/mcx_options_ce.parquet\", engine='pyarrow')\n",
    "files_created.append(f\"{output_folder}/mcx_options_ce.parquet\")\n",
    "\n",
    "df_mcx_pe.to_parquet(f\"{output_folder}/mcx_options_pe.parquet\", engine='pyarrow')\n",
    "files_created.append(f\"{output_folder}/mcx_options_pe.parquet\")\n",
    "\n",
    "print(\"‚úÖ Parquet files created successfully!\")\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   - NSE/NFO Total: {len(df_all):,} instruments\")\n",
    "print(f\"   - MCX Total: {len(df_mcx):,} instruments\")\n",
    "print(f\"   - MCX Futures: {len(df_mcx_futures):,} instruments\")\n",
    "print(f\"   - MCX CE: {len(df_mcx_ce):,} instruments\")\n",
    "print(f\"   - MCX PE: {len(df_mcx_pe):,} instruments\")\n",
    "\n",
    "print(\"\\nüìÅ Files saved to:\")\n",
    "for file in files_created:\n",
    "    full_path = os.path.abspath(file)\n",
    "    file_size = os.path.getsize(full_path) / (1024 * 1024)  # Size in MB\n",
    "    print(f\"   - {file} ({file_size:.2f} MB)\")\n",
    "    \n",
    "print(\"\\nüí° To view in Python:\")\n",
    "print(\"   import pandas as pd\")\n",
    "print(\"   df = pd.read_parquet('instruments_data/equity.parquet')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e444cb3",
   "metadata": {},
   "source": [
    "## 5. Create Instrument Tree (Nested Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85614abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a nested dictionary: { name: { type: dataframe } }\n",
    "# This groups by name first, then by instrument_type within each name\n",
    "instrument_tree = {\n",
    "    name: {inst_type: data for inst_type, data in name_group.groupby('instrument_type')}\n",
    "    for name, name_group in df_all.groupby('name')\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee6018",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ Created instrument_tree with {len(instrument_tree)} unique instrument names\")\n",
    "print(f\"Example: instrument_tree['BSE'] contains: {list(instrument_tree.get('BSE', {}).keys())}\")\n",
    "\n",
    "# Usage:\n",
    "BSE = instrument_tree['BSE']['FUT']\n",
    "pprint(BSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e9269",
   "metadata": {},
   "source": [
    "## 5. Option Chain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe7457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive lookup dictionary for symbol metadata\n",
    "# Format: { 'SYMBOL_NAME': { 'exchange': 'EXCHANGE', 'expiries': [date1, date2, ...] } }\n",
    "\n",
    "symbol_lookup = {}\n",
    "\n",
    "# # 1. Ensure we have MCX data\n",
    "# if 'df_mcx' not in locals() and 'instruments_mcx' in locals():\n",
    "#     print(\"Creating df_mcx from instruments_mcx...\")\n",
    "#     df_mcx = pd.DataFrame(instruments_mcx)\n",
    "#     # Standardize expiry\n",
    "#     if 'expiry' in df_mcx.columns:\n",
    "#          df_mcx['expiry'] = pd.to_datetime(df_mcx['expiry'], errors='coerce')\n",
    "\n",
    "\n",
    "# 2. Process NSE/NFO/MCX Instruments (from df_all)\n",
    "print(\"Building lookup for NSE/NFO symbols...\")\n",
    "\n",
    "if 'df_all' in locals():\n",
    "    for name, group in df_all.groupby('name'):\n",
    "        # Determine the primary exchange for trading (prefer NFO if available, else NSE/Indices)\n",
    "        exchanges = group['exchange'].unique()\n",
    "        \n",
    "        if 'NFO' in exchanges:\n",
    "            primary_exchange = 'NFO'\n",
    "        elif 'NSE' in exchanges:\n",
    "            primary_exchange = 'NSE'\n",
    "        else:\n",
    "            primary_exchange = exchanges[0]\n",
    "            \n",
    "        # extract valid expiries\n",
    "        expiries = sorted(group['expiry'].dropna().unique())\n",
    "        \n",
    "        symbol_lookup[name] = {\n",
    "            'exchange': primary_exchange,\n",
    "            'expiries': expiries\n",
    "        }\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è df_all not found in local variables.\")\n",
    "\n",
    "# 3. Process MCX Instruments (from df_mcx)\n",
    "if 'df_mcx' in locals():\n",
    "    print(\"Building lookup for MCX symbols...\")\n",
    "    for name, group in df_mcx.groupby('name'):\n",
    "        # MCX usually has expiries for everything\n",
    "        expiries = sorted(group['expiry'].dropna().unique())\n",
    "        \n",
    "        symbol_lookup[name] = {\n",
    "            'exchange': 'MCX',\n",
    "            'expiries': expiries\n",
    "        }\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è df_mcx not found. MCX symbols will be missing from lookup.\")\n",
    "\n",
    "print(f\"\\n‚úÖ Symbol Lookup Created for {len(symbol_lookup)} symbols\")\n",
    "\n",
    "# --- TEST VARIFICATION ---\n",
    "print(\"-\" * 50)\n",
    "print(\"TESTING LOOKUP:\")\n",
    "test_symbols = ['GOLDM', 'CRUDEOIL', 'ADANIENT', 'NIFTY', 'RELIANCE']\n",
    "\n",
    "for sym in test_symbols:\n",
    "    if sym in symbol_lookup:\n",
    "        entry = symbol_lookup[sym]\n",
    "        exp_count = len(entry['expiries'])\n",
    "        first_expiry = entry['expiries'][0].date() if exp_count > 0 else \"None\"\n",
    "        print(f\"‚úÖ {sym:<10} | Exch: {entry['exchange']:<5} | Expiries: {exp_count} (Next: {first_expiry})\")\n",
    "    else:\n",
    "        print(f\"‚ùå {sym:<10} | Not found in lookup\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all available expiries for a symbol\n",
    "def get_all_expiries(kite, symbol, exchange=None):\n",
    "    \"\"\"\n",
    "    Get all available expiry dates for a given symbol using the pre-computed lookup table.\n",
    "    \n",
    "    Args:\n",
    "        kite: KiteConnect instance (kept for compatibility, though not used if lookup exists)\n",
    "        symbol: Base symbol name (e.g., \"NIFTY\", \"GOLDM\")\n",
    "        exchange: Exchange name (optional, will be auto-detected from lookup)\n",
    "    \n",
    "    Returns:\n",
    "        List of expiry dates sorted in ascending order\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Try to use the pre-computed symbol_lookup dictionary\n",
    "        if 'symbol_lookup' in globals() and symbol in symbol_lookup:\n",
    "            # print(f\"‚úÖ Found '{symbol}' in lookup table (Exchange: {symbol_lookup[symbol]['exchange']})\")\n",
    "            return symbol_lookup[symbol]['expiries']\n",
    "            \n",
    "        # 2. Fallback: Fetch from kite if lookup not available/symbol not found\n",
    "        print(f\"‚ö†Ô∏è Symbol '{symbol}' not found in lookup table. Fetching from API...\")\n",
    "        \n",
    "        # Determine exchange if not provided\n",
    "        if not exchange:\n",
    "            exchange = \"NFO\" # Default\n",
    "            \n",
    "        instruments = kite.instruments(exchange)\n",
    "        matching = [i for i in instruments if i['name'] == symbol and i['expiry']]\n",
    "        expiries = sorted(list(set(i['expiry'] for i in matching)))\n",
    "        \n",
    "        return expiries\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching expiries: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Example usage help\n",
    "print(\"Function get_all_expiries(kite, symbol) updated to use symbol_lookup table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450042aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_expiries(kite, symbol= 'GOLDM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653baa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ Created instrument_tree with {len(instrument_tree)} unique instrument names\")\n",
    "print(f\"Example: instrument_tree['ADANIENT'] contains: {list(instrument_tree.get('BSE', {}).keys())}\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_option_chain(symbol, expiry):\n",
    "    \"\"\"\n",
    "    Build option chain for a symbol and expiry using global dataframes (df_all/df_mcx) and symbol_lookup.\n",
    "    Automatically selects the correct exchange and DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): Base symbol (e.g. 'NIFTY', 'GOLDM')\n",
    "        expiry (str/date): Expiry date\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Option chain with columns [tradingsymbol, strike, instrument_type, expiry, exchange, etc from source]\n",
    "    \"\"\"\n",
    "    # 1. Select the source Dataframe based on lookup or default\n",
    "    source_df = None\n",
    "    exchange = \"NFO\" # Default fallback\n",
    "    \n",
    "    if 'symbol_lookup' in globals() and symbol in symbol_lookup:\n",
    "        exchange = symbol_lookup[symbol]['exchange']\n",
    "        \n",
    "        # Decide which global DF to use\n",
    "        if exchange == 'MCX' and 'df_mcx' in globals():\n",
    "            source_df = globals()['df_mcx']\n",
    "        elif 'df_all' in globals():\n",
    "            source_df = globals()['df_all']\n",
    "            \n",
    "    else:\n",
    "        # Fallback if lookup fails but df_all exists (assume NFO/NSE)\n",
    "        if 'df_all' in globals():\n",
    "             source_df = globals()['df_all']\n",
    "            \n",
    "    if source_df is None:\n",
    "        print(\"‚ùå Error: No instruments data found (df_all/df_mcx missing in globals)\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # 2. Filter Data\n",
    "    # Ensure expiry is compatible (Timestamp)\n",
    "    try:\n",
    "        expiry = pd.to_datetime(expiry)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # Perform filtering\n",
    "    # We look for: Name matches symbol AND Expiry matches AND Instrument is CE or PE\n",
    "    mask = (\n",
    "        (source_df['name'] == symbol) &\n",
    "        (source_df['expiry'] == expiry) &\n",
    "        (source_df['instrument_type'].isin(['CE', 'PE']))\n",
    "    )\n",
    "    \n",
    "    chain = source_df[mask].copy()\n",
    "    \n",
    "    if chain.empty:\n",
    "        print(f\"‚ö†Ô∏è No options found for {symbol} expiring {expiry.date()} (Exchange: {exchange})\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # Make sure we sort by strike for better view\n",
    "    cols_priority = ['tradingsymbol', 'strike', 'instrument_type', 'expiry', 'lot_size', 'exchange', 'instrument_token']\n",
    "    available_cols = [c for c in cols_priority if c in chain.columns]\n",
    "    \n",
    "    return chain[available_cols].sort_values('strike').reset_index(drop=True)\n",
    "\n",
    "\n",
    "def enrich_with_market_data(kite, option_chain_df):\n",
    "    \"\"\"\n",
    "    Fetch market depth/Limit/OI for the option chain dataframe\n",
    "    \"\"\"\n",
    "    if option_chain_df.empty:\n",
    "        return option_chain_df\n",
    "\n",
    "    # 1. Construct Exchange:Symbol List for kite.quote()\n",
    "    # Uses 'exchange' column if available (from build_option_chain), else defaults to 'NFO'\n",
    "    symbols_to_quote = []\n",
    "    \n",
    "    if 'exchange' in option_chain_df.columns:\n",
    "        # Vectorized string creation is faster\n",
    "        symbols_to_quote = (option_chain_df['exchange'] + \":\" + option_chain_df['tradingsymbol']).tolist()\n",
    "    else:\n",
    "        # Fallback logic\n",
    "        symbols_to_quote = [\"NFO:\" + s for s in option_chain_df['tradingsymbol']]\n",
    "\n",
    "    # 2. Fetch Quotes (Batch)\n",
    "    try:\n",
    "        quotes = kite.quote(symbols_to_quote)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching quotes: {e}\")\n",
    "        return option_chain_df\n",
    "\n",
    "    # 3. Enrich DataFrame with Quote Data\n",
    "    enriched_data = []\n",
    "    \n",
    "    for idx, row in option_chain_df.iterrows():\n",
    "        # Reconstruct key used for lookup\n",
    "        if 'exchange' in row:\n",
    "            key = f\"{row['exchange']}:{row['tradingsymbol']}\"\n",
    "        else:\n",
    "            key = f\"NFO:{row['tradingsymbol']}\"\n",
    "            \n",
    "        quote = quotes.get(key, {})\n",
    "        \n",
    "        # Market Depth Extraction\n",
    "        depth = quote.get('depth', {})\n",
    "        buy_depth = depth.get('buy', [])\n",
    "        sell_depth = depth.get('sell', [])\n",
    "        \n",
    "        # Create row dict and update\n",
    "        item = row.to_dict()\n",
    "        item.update({\n",
    "            'ltp': quote.get('last_price', 0),\n",
    "            'oi': quote.get('oi', 0),\n",
    "            'volume': quote.get('volume', 0),\n",
    "            'bid': buy_depth[0]['price'] if buy_depth else 0,\n",
    "            'ask': sell_depth[0]['price'] if sell_depth else 0,\n",
    "            'bid_qty': buy_depth[0]['quantity'] if buy_depth else 0,\n",
    "            'ask_qty': sell_depth[0]['quantity'] if sell_depth else 0,\n",
    "            'ohlc': quote.get('ohlc', {})\n",
    "        })\n",
    "        enriched_data.append(item)\n",
    "\n",
    "    return pd.DataFrame(enriched_data)\n",
    "\n",
    "print(\"‚úÖ Updated build_option_chain and enrich_with_market_data to use lookup tables and optimized dataframes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipenv shell\n",
    "# Activate pipenv shell to ensure all dependencies are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7629e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def get_underlying_ltp(kite, symbol, expiry, source_df=None):\n",
    "    \"\"\"\n",
    "    Attempts to find the underlying price. \n",
    "    1. Looks for a Future contract expiring ON or AFTER the option expiry (typical for MCX).\n",
    "    2. Fallback to Spot index/equity mapping.\n",
    "    \"\"\"\n",
    "    underlying_ltp = 0\n",
    "    used_symbol = \"\"\n",
    "\n",
    "    # Ensure expiry is datetime\n",
    "    if not isinstance(expiry, pd.Timestamp):\n",
    "        try:f\n",
    "            expiry = pd.to_datetime(expiry)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # 1. Try to find corresponding Future for this expiry if dataframe provided\n",
    "    if source_df is not None and not source_df.empty:\n",
    "        # Find all Futures for this symbol\n",
    "        mask_fut = (source_df['name'] == symbol) & (source_df['instrument_type'] == 'FUT')\n",
    "        futures = source_df[mask_fut].copy()\n",
    "        \n",
    "        if not futures.empty:\n",
    "            # Ensure future expiries are datetime\n",
    "            # (Assuming source_df is standardized, but safety check)\n",
    "            if 'expiry' in futures.columns: # extra safety\n",
    "                 futures['expiry'] = pd.to_datetime(futures['expiry'])\n",
    "            \n",
    "            # Sort by expiry\n",
    "            futures = futures.sort_values('expiry')\n",
    "            \n",
    "            # Filter: Future Expiry >= Option Expiry\n",
    "            # Logic: Option settles into the nearest future that is active at option expiry\n",
    "            valid_futures = futures[futures['expiry'] >= expiry]\n",
    "            \n",
    "            if not valid_futures.empty:\n",
    "                # Pick the nearest valid future (First one)\n",
    "                fut_row = valid_futures.iloc[0]\n",
    "                \n",
    "                tradingsymbol = fut_row['tradingsymbol']\n",
    "                exch = fut_row['exchange'] \n",
    "                fut_expiry = fut_row['expiry']\n",
    "                \n",
    "                instrument_token = f\"{exch}:{tradingsymbol}\"\n",
    "                try:\n",
    "                    ltp_resp = kite.ltp(instrument_token)\n",
    "                    if instrument_token in ltp_resp:\n",
    "                        underlying_ltp = ltp_resp[instrument_token]['last_price']\n",
    "                        used_symbol = instrument_token\n",
    "                        print(f\"üîπ Underlying identified as FUTURE: {used_symbol} (Exp: {fut_expiry.date()}) | Option Exp: {expiry.date()}\")\n",
    "                        return underlying_ltp\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No future found expiring on/after {expiry.date()}\")\n",
    "\n",
    "    # 2. Fallback to Spot (Indices or Stocks)\n",
    "    index_map = {\n",
    "        'NIFTY': 'NSE:NIFTY 50',\n",
    "        'BANKNIFTY': 'NSE:NIFTY BANK',\n",
    "        'FINNIFTY': 'NSE:NIFTY FIN SERVICE'\n",
    "    }\n",
    "    \n",
    "    if symbol in index_map:\n",
    "        used_symbol = index_map[symbol]\n",
    "    else:\n",
    "        # Assume it's a stock on NSE if not NIFTY/BANKNIFTY and Future lookup failed/wasn't applicable\n",
    "        # This is a broad assumption; might need refinement for BSE\n",
    "        used_symbol = f\"NSE:{symbol}\"\n",
    "    \n",
    "    try:\n",
    "        ltp_resp = kite.ltp(used_symbol)\n",
    "        if used_symbol in ltp_resp:\n",
    "            underlying_ltp = ltp_resp[used_symbol]['last_price']\n",
    "            print(f\"üîπ Underlying identified as SPOT: {used_symbol} (LTP: {underlying_ltp})\")\n",
    "        else:\n",
    "             print(f\"‚ö†Ô∏è Could not fetch LTP for {used_symbol} (Spot)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching underlying {used_symbol}: {e}\")\n",
    "        \n",
    "    return underlying_ltp\n",
    "\n",
    "\n",
    "def implied_volatility(price, S, K, T, r, flag):\n",
    "    \"\"\"\n",
    "    Finds implied volatility using Newton-Raphson method.\n",
    "    \"\"\"\n",
    "    MAX_ITER = 100\n",
    "    PRECISION = 1.0e-5\n",
    "    sigma = 0.5 # Initial guess\n",
    "    \n",
    "    for i in range(MAX_ITER):\n",
    "        # Calculate BS Price\n",
    "        d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "        d2 = d1 - sigma * np.sqrt(T)\n",
    "        \n",
    "        if flag == 'CE':\n",
    "            bs_price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "            vega = S * np.sqrt(T) * norm.pdf(d1)\n",
    "        else:\n",
    "            bs_price = K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
    "            vega = S * np.sqrt(T) * norm.pdf(d1)\n",
    "            \n",
    "        diff = price - bs_price\n",
    "        \n",
    "        if abs(diff) < PRECISION:\n",
    "            return sigma\n",
    "        \n",
    "        if abs(vega) < 1e-8: # Avoid division by zero\n",
    "             return sigma # Return current best guess or 0.0\n",
    "             \n",
    "        sigma = sigma + (diff / vega)\n",
    "        \n",
    "    return sigma\n",
    "\n",
    "def add_iv_columns(chain, underlying_ltp, risk_free_rate=0.10):\n",
    "    ivs = []\n",
    "    today = pd.Timestamp.now()\n",
    "    \n",
    "    for _, row in chain.iterrows():\n",
    "        try:\n",
    "            # Inputs\n",
    "            price = row['ltp']\n",
    "            strike = row['strike']\n",
    "            expiry = row['expiry']\n",
    "            # If price is 0, IV is 0\n",
    "            if price <= 0:\n",
    "                ivs.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # Time to expiry\n",
    "            if not isinstance(expiry, pd.Timestamp):\n",
    "                expiry = pd.to_datetime(expiry)\n",
    "                \n",
    "            # Set time to end of expiry day (approx)\n",
    "            expiry_end = expiry + pd.Timedelta(hours=15, minutes=30)\n",
    "            \n",
    "            # Difference in years\n",
    "            delta = expiry_end - today\n",
    "            T = delta.total_seconds() / (365.0 * 24 * 3600)\n",
    "            \n",
    "            if T <= 0.001: # Expiring now/expired\n",
    "                T = 0.001\n",
    "            \n",
    "            iv = implied_volatility(\n",
    "                price, \n",
    "                underlying_ltp, \n",
    "                strike, \n",
    "                T, \n",
    "                risk_free_rate, \n",
    "                row['instrument_type']\n",
    "            )\n",
    "            ivs.append(iv * 100) # Percentage\n",
    "        except Exception:\n",
    "            ivs.append(0.0)\n",
    "            \n",
    "    chain['iv'] = ivs\n",
    "    return chain\n",
    "\n",
    "print(\"‚úÖ IV Calculation Functions (Black-Scholes) added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279bd780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65cdfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e28b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test kite.quote() with a list of symbols\n",
    "symbols = [\"NIFTY26FEB24000CE\", \"NIFTY26FEB24000PE\", \"NIFTY26FEB23900CE\"]\n",
    "\n",
    "quotes = kite.quote(symbols)\n",
    "pprint(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new functions and IV Calculation\n",
    "symbol = \"GOLDM\"\n",
    "\n",
    "# 1. Get available expiries\n",
    "available_expiries = get_all_expiries(kite, symbol)\n",
    "\n",
    "if len(available_expiries) > 0:\n",
    "    test_expiry = available_expiries[0]\n",
    "    print(f\"Testing with Symbol: {symbol}, Expiry: {test_expiry.date()}\")\n",
    "    \n",
    "    # 2. Build and enrich chain\n",
    "    chain = build_option_chain(symbol, test_expiry)\n",
    "    chain = enrich_with_market_data(kite, chain)\n",
    "\n",
    "    if not chain.empty:\n",
    "        # 3. Determine Source Dataframe for Futures lookup\n",
    "        exch = chain.iloc[0]['exchange'] if 'exchange' in chain.columns else 'NFO'\n",
    "        source_df_for_ltp = df_mcx if exch == 'MCX' and 'df_mcx' in globals() else (df_all if 'df_all' in globals() else None)\n",
    "        \n",
    "        # 4. Get Underlying LTP\n",
    "        underlying_ltp = get_underlying_ltp(kite, symbol, test_expiry, source_df_for_ltp)\n",
    "        \n",
    "        if underlying_ltp > 0:\n",
    "            # 5. Calculate IV\n",
    "            chain = add_iv_columns(chain, underlying_ltp)\n",
    "            print(f\"‚úÖ IV Calculated using Underlying LTP: {underlying_ltp}\")\n",
    "            \n",
    "            # 6. Show Results (Filter interesting columns)\n",
    "            cols = ['tradingsymbol', 'strike', 'instrument_type', 'ltp', 'iv', 'oi', 'volume']\n",
    "            display_cols = [c for c in cols if c in chain.columns]\n",
    "            \n",
    "            # Show ATMish options (around underlying price)\n",
    "            chain['diff_pct'] = abs(chain['strike'] - underlying_ltp) / underlying_ltp\n",
    "            atm_view = chain.sort_values('diff_pct').head(10).sort_values('strike')\n",
    "            \n",
    "            print(\"\\nüìä Option Chain with IV (ATM View):\")\n",
    "            print(atm_view[display_cols].to_string(index=False))\n",
    "        else:\n",
    "            print(\"‚ùå Skipping IV calculation (Underlying LTP not found)\")\n",
    "            print(chain[['tradingsymbol', 'strike', 'ltp']].head())\n",
    "            \n",
    "    else:\n",
    "        print(\"Chain is empty for this expiry.\")\n",
    "else:\n",
    "    print(f\"‚ùå No expiries found for {symbol}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol= 'GOLDM'\n",
    "test_expiry = get_all_expiries(kite, symbol)[1]\n",
    "\n",
    "chain = build_option_chain(symbol, test_expiry)\n",
    "chain = enrich_with_market_data(kite, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c26af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IV for the existing 'chain' dataframe\n",
    "# Using the global 'chain' variable from previous generation\n",
    "\n",
    "if 'chain' in locals() and not chain.empty and 'symbol' in locals():\n",
    "    print(f\"Running IV Calculation for symbol: {symbol}...\")\n",
    "    \n",
    "    # 1. Identify Expiry and Exchange from the chain data itself\n",
    "    chain_expiry = chain['expiry'].iloc[0]\n",
    "    exchange = chain['exchange'].iloc[0] if 'exchange' in chain.columns else 'NFO'\n",
    "    \n",
    "    print(f\"   Expiry: {chain_expiry} | Exchange: {exchange}\")\n",
    "\n",
    "    # 1.5 Ensure Market Data (LTP) is present\n",
    "    if 'ltp' not in chain.columns:\n",
    "        print(\"‚ö†Ô∏è Market data (LTP) missing from chain. Fetching quotes now...\")\n",
    "        chain = enrich_with_market_data(kite, chain)\n",
    "        if 'ltp' not in chain.columns:\n",
    "             print(\"‚ùå Failed to fetch market data. Aborting IV calculation.\")\n",
    "             # Stop usage of chain here if critical\n",
    "    \n",
    "    # 2. Select Source DataFrame for Future Price lookup\n",
    "    # (MCX futures are in df_mcx, NFO futures in df_all)\n",
    "    source_df = None\n",
    "    if exchange == 'MCX' and 'df_mcx' in globals():\n",
    "        source_df = df_mcx\n",
    "    elif 'df_all' in globals():\n",
    "        source_df = df_all\n",
    "        \n",
    "    # 3. Fetch Underlying LTP (Spot or Future)\n",
    "    underlying_ltp = get_underlying_ltp(kite, symbol, chain_expiry, source_df)\n",
    "    \n",
    "    if underlying_ltp > 0 and 'ltp' in chain.columns:\n",
    "        # 4. Apply Black-Scholes IV Calculation\n",
    "        chain = add_iv_columns(chain, underlying_ltp)\n",
    "        print(f\"‚úÖ IV Calculated successfully! Underlying Ref Price: {underlying_ltp}\")\n",
    "        \n",
    "        # 5. Display ATM Options (formatted)\n",
    "        # Find ATM by smallest distance to underlying price\n",
    "        chain['diff_pct'] = abs(chain['strike'] - underlying_ltp) / underlying_ltp\n",
    "        atm_view = chain.sort_values('diff_pct').head(10).sort_values('strike')\n",
    "        \n",
    "        cols = ['tradingsymbol', 'strike', 'instrument_type', 'ltp', 'iv', 'oi', 'volume']\n",
    "        display_cols = [c for c in cols if c in chain.columns]\n",
    "        \n",
    "        print(\"\\nüìä Option Chain with IV (ATM View):\")\n",
    "        print(atm_view[display_cols].to_string(index=False))\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Failed to get Underlying LTP or Market Data. Cannot calculate IV.\")\n",
    "else:\n",
    "    print(\"‚ùå 'chain' dataframe or 'symbol' variable is missing. Please run the chain generation cell above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82922db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Inspect MCX Futures for GOLDM to fix underlying lookup\n",
    "print(\"DEBUG: Checking MCX Futures for GOLDM\")\n",
    "if 'df_mcx' in globals():\n",
    "    mcx_futs = df_mcx[(df_mcx['name'] == 'GOLDM') & (df_mcx['instrument_type'] == 'FUT')]\n",
    "    print(mcx_futs[['tradingsymbol', 'expiry', 'instrument_type']].sort_values('expiry'))\n",
    "    \n",
    "    print(f\"\\nTarget Expiry from Option Chain: {chain['expiry'].iloc[0]}\")\n",
    "else:\n",
    "    print(\"df_mcx not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c79db",
   "metadata": {},
   "source": [
    "## üß† Memory Usage Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12571e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üíæ SYSTEM MEMORY\n",
      "================================================================================\n",
      "Total RAM:        7.82 GB\n",
      "Available:        2.20 GB\n",
      "Used:             5.62 GB (71.9%)\n",
      "Free:             2.20 GB\n",
      "\n",
      "================================================================================\n",
      "üêç PYTHON PROCESS MEMORY\n",
      "================================================================================\n",
      "RSS (Resident):   0.12 GB\n",
      "VMS (Virtual):    0.38 GB\n",
      "\n",
      "================================================================================\n",
      "üìä DATAFRAME & DICTIONARY MEMORY USAGE (Auto-detected)\n",
      "================================================================================\n",
      "\n",
      "üîπ No DataFrames found in globals()\n",
      "\n",
      "================================================================================\n",
      "TOTAL Data Structure Memory:            0.00 MB\n",
      "Number of DataFrames:                      0\n",
      "Number of Large Dictionaries:              0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"\n",
    "    Display comprehensive memory usage statistics\n",
    "    Automatically detects all pandas DataFrames and large dictionaries in globals()\n",
    "    \"\"\"\n",
    "    # System Memory\n",
    "    mem = psutil.virtual_memory()\n",
    "    \n",
    "    # Python Process Memory\n",
    "    process = psutil.Process()\n",
    "    process_mem = process.memory_info()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üíæ SYSTEM MEMORY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total RAM:        {mem.total / (1024**3):.2f} GB\")\n",
    "    print(f\"Available:        {mem.available / (1024**3):.2f} GB\")\n",
    "    print(f\"Used:             {mem.used / (1024**3):.2f} GB ({mem.percent}%)\")\n",
    "    print(f\"Free:             {mem.free / (1024**3):.2f} GB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üêç PYTHON PROCESS MEMORY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"RSS (Resident):   {process_mem.rss / (1024**3):.2f} GB\")\n",
    "    print(f\"VMS (Virtual):    {process_mem.vms / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # AUTO-DETECT DataFrames and Dictionaries from globals()\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä DATAFRAME & DICTIONARY MEMORY USAGE (Auto-detected)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Collect all DataFrames and large dictionaries\n",
    "    dataframes = []\n",
    "    dictionaries = []\n",
    "    \n",
    "    for name, obj in globals().items():\n",
    "        # Skip private/system variables and modules\n",
    "        if name.startswith('_') or name in ['In', 'Out', 'get_ipython', 'exit', 'quit']:\n",
    "            continue\n",
    "            \n",
    "        if isinstance(obj, pd.DataFrame):\n",
    "            mem_usage = obj.memory_usage(deep=True).sum() / (1024**2)\n",
    "            dataframes.append((name, obj, mem_usage))\n",
    "        elif isinstance(obj, dict) and sys.getsizeof(obj) > 1024:  # Only dicts > 1KB\n",
    "            mem_usage = sys.getsizeof(obj) / (1024**2)\n",
    "            dictionaries.append((name, obj, mem_usage))\n",
    "    \n",
    "    # Sort by memory usage (largest first)\n",
    "    dataframes.sort(key=lambda x: x[2], reverse=True)\n",
    "    dictionaries.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    total_df_memory = 0\n",
    "    \n",
    "    # Display DataFrames\n",
    "    if dataframes:\n",
    "        print(\"\\nüîπ DATAFRAMES:\")\n",
    "        print(f\"{'Name':<25} {'Rows':>12} {'Columns':>8} {'Memory':>12}\")\n",
    "        print(\"-\" * 80)\n",
    "        for name, df, mem_usage in dataframes:\n",
    "            total_df_memory += mem_usage\n",
    "            print(f\"{name:<25} {len(df):>12,} {len(df.columns):>8} {mem_usage:>10.2f} MB\")\n",
    "    else:\n",
    "        print(\"\\nüîπ No DataFrames found in globals()\")\n",
    "    \n",
    "    # Display Dictionaries\n",
    "    if dictionaries:\n",
    "        print(\"\\nüîπ DICTIONARIES:\")\n",
    "        print(f\"{'Name':<25} {'Items':>12} {'Memory':>12}\")\n",
    "        print(\"-\" * 80)\n",
    "        for name, d, mem_usage in dictionaries:\n",
    "            total_df_memory += mem_usage\n",
    "            print(f\"{name:<25} {len(d):>12,} {mem_usage:>10.2f} MB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{'TOTAL Data Structure Memory:':<33} {total_df_memory:>10.2f} MB\")\n",
    "    print(f\"{'Number of DataFrames:':<33} {len(dataframes):>10}\")\n",
    "    print(f\"{'Number of Large Dictionaries:':<33} {len(dictionaries):>10}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return {\n",
    "        'system_total_gb': mem.total / (1024**3),\n",
    "        'system_available_gb': mem.available / (1024**3),\n",
    "        'system_used_percent': mem.percent,\n",
    "        'process_rss_gb': process_mem.rss / (1024**3),\n",
    "        'dataframes_mb': total_df_memory,\n",
    "        'num_dataframes': len(dataframes),\n",
    "        'num_dictionaries': len(dictionaries)\n",
    "    }\n",
    "\n",
    "# Run memory check\n",
    "memory_stats = get_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "347f92db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory monitoring utilities loaded:\n",
      "   - get_memory_usage() - Full memory report\n",
      "   - mem_snapshot(label) - Quick checkpoint\n",
      "   - @memory_monitor_decorator - Track function memory usage\n"
     ]
    }
   ],
   "source": [
    "def memory_monitor_decorator(func):\n",
    "    \"\"\"\n",
    "    Decorator to monitor memory before and after function execution\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Memory before\n",
    "        process = psutil.Process()\n",
    "        mem_before = process.memory_info().rss / (1024**2)\n",
    "        \n",
    "        # Execute function\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # Memory after\n",
    "        mem_after = process.memory_info().rss / (1024**2)\n",
    "        mem_diff = mem_after - mem_before\n",
    "        \n",
    "        print(f\"\\nüìä Memory Impact for {func.__name__}:\")\n",
    "        print(f\"   Before: {mem_before:.2f} MB\")\n",
    "        print(f\"   After:  {mem_after:.2f} MB\")\n",
    "        print(f\"   Change: {mem_diff:+.2f} MB\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Quick memory snapshot function\n",
    "def mem_snapshot(label=\"\"):\n",
    "    \"\"\"Quick memory snapshot with optional label\"\"\"\n",
    "    process = psutil.Process()\n",
    "    mem_mb = process.memory_info().rss / (1024**2)\n",
    "    sys_mem = psutil.virtual_memory()\n",
    "    \n",
    "    print(f\"üì∏ {label}\")\n",
    "    print(f\"   Process: {mem_mb:.2f} MB | System: {sys_mem.percent}% used\")\n",
    "    return mem_mb\n",
    "\n",
    "print(\"‚úÖ Memory monitoring utilities loaded:\")\n",
    "print(\"   - get_memory_usage() - Full memory report\")\n",
    "print(\"   - mem_snapshot(label) - Quick checkpoint\")\n",
    "print(\"   - @memory_monitor_decorator - Track function memory usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a45b0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ cleanup_memory() function ready\n"
     ]
    }
   ],
   "source": [
    "# Cleanup utility to free memory\n",
    "def cleanup_memory():\n",
    "    \"\"\"\n",
    "    Force garbage collection and report freed memory\n",
    "    \"\"\"\n",
    "    process = psutil.Process()\n",
    "    mem_before = process.memory_info().rss / (1024**2)\n",
    "    \n",
    "    # Force garbage collection\n",
    "    collected = gc.collect()\n",
    "    \n",
    "    mem_after = process.memory_info().rss / (1024**2)\n",
    "    freed = mem_before - mem_after\n",
    "    \n",
    "    print(f\"üßπ Garbage Collection Complete\")\n",
    "    print(f\"   Objects collected: {collected}\")\n",
    "    print(f\"   Memory freed: {freed:.2f} MB\")\n",
    "    print(f\"   Current usage: {mem_after:.2f} MB\")\n",
    "    \n",
    "    return freed\n",
    "\n",
    "print(\"‚úÖ cleanup_memory() function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f142de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ analyze_dataframe_memory(df_name) function ready\n",
      "   Usage: analyze_dataframe_memory('df_all')  # Analyze specific DataFrame\n",
      "          analyze_dataframe_memory()          # Analyze ALL DataFrames\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataframe_memory(df_name=None):\n",
    "    \"\"\"\n",
    "    Analyze memory usage of a specific DataFrame or all DataFrames\n",
    "    Provides column-level breakdown and optimization suggestions\n",
    "    \n",
    "    Args:\n",
    "        df_name (str): Name of DataFrame to analyze. If None, analyzes all.\n",
    "    \"\"\"\n",
    "    if df_name:\n",
    "        # Analyze specific DataFrame\n",
    "        if df_name not in globals():\n",
    "            print(f\"‚ùå DataFrame '{df_name}' not found in globals()\")\n",
    "            return\n",
    "        \n",
    "        df = globals()[df_name]\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"‚ùå '{df_name}' is not a DataFrame\")\n",
    "            return\n",
    "            \n",
    "        dfs_to_analyze = [(df_name, df)]\n",
    "    else:\n",
    "        # Analyze all DataFrames\n",
    "        dfs_to_analyze = [\n",
    "            (name, obj) for name, obj in globals().items()\n",
    "            if isinstance(obj, pd.DataFrame) and not name.startswith('_')\n",
    "        ]\n",
    "    \n",
    "    if not dfs_to_analyze:\n",
    "        print(\"‚ùå No DataFrames found to analyze\")\n",
    "        return\n",
    "    \n",
    "    for name, df in dfs_to_analyze:\n",
    "        total_mb = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"üìä MEMORY ANALYSIS: {name}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total Memory: {total_mb:.2f} MB | Rows: {len(df):,} | Columns: {len(df.columns)}\")\n",
    "        \n",
    "        # Column-level breakdown\n",
    "        print(f\"\\n{'Column':<30} {'Type':<15} {'Memory (MB)':>12} {'%':>8}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        mem_per_col = df.memory_usage(deep=True)\n",
    "        for col in df.columns:\n",
    "            col_mem = mem_per_col[col] / (1024**2)\n",
    "            pct = (col_mem / total_mb) * 100\n",
    "            dtype = str(df[col].dtype)\n",
    "            print(f\"{col:<30} {dtype:<15} {col_mem:>12.2f} {pct:>7.1f}%\")\n",
    "        \n",
    "        # Optimization suggestions\n",
    "        print(\"\\nüí° OPTIMIZATION SUGGESTIONS:\")\n",
    "        suggestions = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            dtype = df[col].dtype\n",
    "            \n",
    "            # Check for object dtype (usually strings)\n",
    "            if dtype == 'object':\n",
    "                unique_ratio = df[col].nunique() / len(df)\n",
    "                if unique_ratio < 0.5:\n",
    "                    suggestions.append(f\"  ‚Ä¢ '{col}': Convert to 'category' (currently object, {unique_ratio:.1%} unique)\")\n",
    "            \n",
    "            # Check for int64 that could be smaller\n",
    "            elif dtype == 'int64':\n",
    "                col_max = df[col].max()\n",
    "                col_min = df[col].min()\n",
    "                if col_min >= 0 and col_max < 65536:\n",
    "                    suggestions.append(f\"  ‚Ä¢ '{col}': Use 'uint16' instead of int64 (max value: {col_max})\")\n",
    "                elif col_min >= -32768 and col_max < 32767:\n",
    "                    suggestions.append(f\"  ‚Ä¢ '{col}': Use 'int16' instead of int64\")\n",
    "                elif col_min >= 0 and col_max < 4294967296:\n",
    "                    suggestions.append(f\"  ‚Ä¢ '{col}': Use 'uint32' instead of int64\")\n",
    "            \n",
    "            # Check for float64 that could be float32\n",
    "            elif dtype == 'float64':\n",
    "                suggestions.append(f\"  ‚Ä¢ '{col}': Consider 'float32' if precision allows (50% memory saving)\")\n",
    "        \n",
    "        if suggestions:\n",
    "            for s in suggestions[:10]:  # Show top 10\n",
    "                print(s)\n",
    "        else:\n",
    "            print(\"  ‚úÖ DataFrame is already well-optimized!\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "\n",
    "print(\"‚úÖ analyze_dataframe_memory(df_name) function ready\")\n",
    "print(\"   Usage: analyze_dataframe_memory('df_all')  # Analyze specific DataFrame\")\n",
    "print(\"          analyze_dataframe_memory()          # Analyze ALL DataFrames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4ce94cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå DataFrame 'df_all' not found in globals()\n"
     ]
    }
   ],
   "source": [
    "analyze_dataframe_memory('df_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afad5c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Just get DataFrame names\n",
    "df_names = [name for name, obj in globals().items() if isinstance(obj, pd.DataFrame)]\n",
    "print(df_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c88d6",
   "metadata": {},
   "source": [
    "## 14. SQLite Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Create a SQLite database (creates file if it doesn't exist)\n",
    "db_path = \"trading_data.db\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(f\"Connected to SQLite database: {os.path.abspath(db_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1971e02",
   "metadata": {},
   "source": [
    "## 15. Create Database Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bfd480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample table for storing trades\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS trades (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        symbol TEXT NOT NULL,\n",
    "        trade_type TEXT NOT NULL,\n",
    "        quantity INTEGER NOT NULL,\n",
    "        price REAL NOT NULL,\n",
    "        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Create a table for storing holdings snapshots\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS holdings_snapshot (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        symbol TEXT NOT NULL,\n",
    "        quantity INTEGER,\n",
    "        average_price REAL,\n",
    "        last_price REAL,\n",
    "        pnl REAL,\n",
    "        snapshot_time DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "print(\"Tables created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d44aa8c",
   "metadata": {},
   "source": [
    "## 16. Insert Sample Trade Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46799b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Insert a sample trade\n",
    "cursor.execute('''\n",
    "    INSERT INTO trades (symbol, trade_type, quantity, price)\n",
    "    VALUES (?, ?, ?, ?)\n",
    "''', ('NSE:RELIANCE', 'BUY', 10, 2450.50))\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Query all trades\n",
    "cursor.execute('SELECT * FROM trades')\n",
    "trades = cursor.fetchall()\n",
    "print(\"All trades:\")\n",
    "for trade in trades:\n",
    "    print(trade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff12cd",
   "metadata": {},
   "source": [
    "## 17. Database Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to close the database connection\n",
    "def close_db():\n",
    "    conn.close()\n",
    "    print(\"Database connection closed.\")\n",
    "\n",
    "# Uncomment to close when done:\n",
    "# close_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc4f69",
   "metadata": {},
   "source": [
    "## 18. Get LTP and Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d9fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LTP (Last Traded Price) for a single stock\n",
    "ltp = kite.ltp(\"NSE:RELIANCE\")\n",
    "pprint(ltp)\n",
    "\n",
    "# Or get LTP for multiple instruments\n",
    "ltps = kite.ltp([\"NSE:RELIANCE\", \"NSE:INFY\", \"NSE:TCS\"])\n",
    "pprint(ltps)\n",
    "\n",
    "# Get detailed quote with more data\n",
    "quote = kite.quote(\"NSE:RELIANCE\")\n",
    "pprint(quote)\n",
    "\n",
    "# Get multiple quotes\n",
    "quotes = kite.quote([\"NSE:RELIANCE\", \"NSE:INFY\"])\n",
    "pprint(quotes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49426eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b4c60a6",
   "metadata": {},
   "source": [
    "## 19. API Rate Limit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e60ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# API Rate Limit Test\n",
    "counter = 1\n",
    "try:\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Make API calls\n",
    "        positions = kite.positions()\n",
    "        orders = kite.orders()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        \n",
    "        # Clear previous output and print current stats\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Request #{counter}\")\n",
    "        print(f\"Time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(f\"Response Time: {elapsed:.3f} seconds\")\n",
    "        print(f\"Positions retrieved: {len(positions.get('net', []))} net, {len(positions.get('day', []))} day\")\n",
    "        print(f\"Orders retrieved: {len(orders)}\")\n",
    "        print(\"\\nPress Ctrl+C to stop...\")\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        # Sleep for 0.1 second before next request\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n\\nStopped after {counter-1} requests\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\nError occurred after {counter-1} requests:\")\n",
    "    print(f\"Error Type: {type(e).__name__}\")\n",
    "    print(f\"Error Message: {str(e)}\")\n",
    "    print(f\"Time: {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_Algoarms-ktqofeKa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
